{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "responsible-alcohol",
   "metadata": {},
   "source": [
    "1. Intro: Hvad vil vi?\n",
    "    - Prædiktere om kommentarer bliver upvoted\n",
    "    - Behandler signifikante ord/nøgleord som variable\n",
    "2. Alternative tokenizers\n",
    "    - effektivitet vs. nøjagtighed\n",
    "3. Sammenligning af tokenizers\n",
    "4. Ordoptælling med scikit-learn\n",
    "5. Alternative vægtninger i optællinger (TF-IDF)\n",
    "6. Fra tekstdata til variable/features\n",
    "    - Fra score til dummy\n",
    "    - Fra tekst til dummies\n",
    "\n",
    "- Supplerende: Fra rådata til tabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-morrison",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-external",
   "metadata": {},
   "source": [
    "# Alternative tokenizers\n",
    "\n",
    "- stanza kun med tokenizer\n",
    "- spacy kun med tokenizer\n",
    "- sklearn CountVectorizer().build_tokenizer()\n",
    "\n",
    "- Tidssammenligning\n",
    "    - stanza med det hele\n",
    "    - stanza tokenize\n",
    "    - spacy tokenize\n",
    "    - sklearn CountVectorizer().build_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-tennessee",
   "metadata": {},
   "source": [
    "## stanza - med det hele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfactory-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download ressourcer\n",
    "#nltk.download('stopwords')\n",
    "#stanza.download('da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grave-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "redditdata_url = \"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark-comments_01032021-08032021_long.csv\"\n",
    "reddit_df = pd.read_csv(redditdata_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "bottom-stockholm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 12:24:38 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "| lemma     | ddt     |\n",
      "| depparse  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-17 12:24:38 INFO: Use device: cpu\n",
      "2021-03-17 12:24:38 INFO: Loading: tokenize\n",
      "2021-03-17 12:24:38 INFO: Loading: pos\n",
      "2021-03-17 12:24:39 INFO: Loading: lemma\n",
      "2021-03-17 12:24:39 INFO: Loading: depparse\n",
      "2021-03-17 12:24:40 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Definer tokenizer\n",
    "\n",
    "nlp = stanza.Pipeline('da')\n",
    "\n",
    "def tokenizer_stanza(text): # Definerer funktion ud fra koden fra tidligere    \n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.lemma) < 2):\n",
    "                continue\n",
    "            if (word.pos in pos_tags) and (word.lemma not in stop_words):\n",
    "                tokens.append(word.lemma)\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "promotional-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-fashion",
   "metadata": {},
   "source": [
    "## stanza - kun tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "powered-basket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:29:38 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-17 13:29:38 INFO: Use device: cpu\n",
      "2021-03-17 13:29:38 INFO: Loading: tokenize\n",
      "2021-03-17 13:29:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Definer tokenizer\n",
    "\n",
    "nlp = stanza.Pipeline('da', processors = 'tokenize')\n",
    "\n",
    "def tokenizer_stanza_simple(text): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.text) < 2):\n",
    "                continue\n",
    "            if word.text.lower() not in stop_words:\n",
    "                tokens.append(word.text.lower())\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "satisfied-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "graphic-cleaner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480         [kl., morgenen, morgen, kl., 6., 00-06, nat.]\n",
       "2925    [må, kun, købe, ammunition, våben, tilladelse,...\n",
       "1100    [særligt, dør, corona, kan, få, god, behandlin...\n",
       "2777                           [husk, må, offentligheden]\n",
       "636     [okay, lad, rette, udmelding, damage, bestemt,...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-dakota",
   "metadata": {},
   "source": [
    "## spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ready-hardware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting da-core-news-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/da_core_news_sm-3.0.0/da_core_news_sm-3.0.0-py3-none-any.whl (18.8 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from da-core-news-sm==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (3.0.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: pathy in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (4.56.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.26.3)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from pathy->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.2.1)\n",
      "Requirement already satisfied: boto3 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.17.20)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.20 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.20.22)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from botocore<1.21.0,>=1.20.20->boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programs\\anaconda3\\envs\\lda\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.20->boto3->smart-open<4.0.0,>=2.2.0->pathy->spacy<3.1.0,>=3.0.0->da-core-news-sm==3.0.0) (1.15.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('da_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "!python -m spacy download da_core_news_sm # download sprogmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "secure-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "def tokenizer_spacy_simple(text): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "\n",
    "    doc = nlp.tokenizer(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.text) < 2):\n",
    "            continue\n",
    "        if word.text.lower() not in stop_words:\n",
    "            tokens.append(word.text.lower())\n",
    "\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "welsh-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_spacy_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "twelve-consideration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480     [kl., morgenen,  \\n, morgen, kl., 6.,  \\n, 00-...\n",
       "2925    [må, kun, købe, ammunition, våben, tilladelse,...\n",
       "1100    [særligt, dør, corona, kan, få, god, behandlin...\n",
       "2777                           [husk, må, offentligheden]\n",
       "636     [okay, lad, rette, udmelding, damage, bestemt,...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-journalist",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "induced-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "def tokenizer_sklearn(text):\n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "    \n",
    "    words = tokenizer(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (len(word) < 2):\n",
    "            continue\n",
    "        if word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "naval-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "characteristic-worth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480               [kl, morgenen, morgen, kl, 00, 06, nat]\n",
       "2925    [må, kun, købe, ammunition, våben, tilladelse,...\n",
       "1100    [særligt, dør, corona, kan, få, god, behandlin...\n",
       "2777                           [husk, må, offentligheden]\n",
       "636     [okay, lad, rette, udmelding, damage, bestemt,...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-belgium",
   "metadata": {},
   "source": [
    "## Tidstælling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "realistic-disney",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:50:31 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "| lemma     | ddt     |\n",
      "| depparse  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-17 13:50:31 INFO: Use device: cpu\n",
      "2021-03-17 13:50:31 INFO: Loading: tokenize\n",
      "2021-03-17 13:50:31 INFO: Loading: pos\n",
      "2021-03-17 13:50:32 INFO: Loading: lemma\n",
      "2021-03-17 13:50:32 INFO: Loading: depparse\n",
      "2021-03-17 13:50:33 INFO: Done loading processors!\n",
      "2021-03-17 13:50:33 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-17 13:50:33 INFO: Use device: cpu\n",
      "2021-03-17 13:50:33 INFO: Loading: tokenize\n",
      "2021-03-17 13:50:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "\n",
    "nlp_stanza = stanza.Pipeline('da')\n",
    "nlp_stanza_simple = stanza.Pipeline('da', processors = 'tokenize')\n",
    "nlp_spacy_simple = spacy.load(\"da_core_news_sm\")\n",
    "sklearn_tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "def tokenizer_stanza(text, nlp = nlp_stanza): # Definerer funktion ud fra koden fra tidligere    \n",
    "\n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.lemma) < 2):\n",
    "                continue\n",
    "            if (word.pos in pos_tags) and (word.lemma not in stop_words):\n",
    "                tokens.append(word.lemma)\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "def tokenizer_stanza_simple(text, nlp = nlp_stanza_simple): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.text) < 2):\n",
    "                continue\n",
    "            if word.text.lower() not in stop_words:\n",
    "                tokens.append(word.text.lower())\n",
    "                \n",
    "    return(tokens)\n",
    "\n",
    "def tokenizer_spacy_simple(text, nlp = nlp_spacy_simple): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "\n",
    "    doc = nlp.tokenizer(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.text) < 2):\n",
    "            continue\n",
    "        if word.text.lower() not in stop_words:\n",
    "            tokens.append(word.text.lower())\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "def tokenizer_sklearn(text, tokenizer = sklearn_tokenizer):\n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "    \n",
    "    words = tokenizer(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (len(word) < 2):\n",
    "            continue\n",
    "        if word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    \n",
    "    return(tokens)\n",
    "\n",
    "def stanza_full_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza)\n",
    "    \n",
    "    print(\"stanza full: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def stanza_simple_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza_simple)\n",
    "    \n",
    "    print(\"stanza simple: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def spacy_simple_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_spacy_simple)\n",
    "    \n",
    "    print(\"spacy simple: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def sklearn_tester():\n",
    "    start_time = time.time()\n",
    "          \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_sklearn)\n",
    "    \n",
    "    print(\"sklearn: {0:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "aggressive-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza full: 18.86 seconds\n",
      "stanza simple: 6.63 seconds\n",
      "spacy simple: 0.13 seconds\n",
      "sklearn: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "stanza_full_tester()\n",
    "stanza_simple_tester()\n",
    "spacy_simple_tester()\n",
    "sklearn_tester()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-humidity",
   "metadata": {},
   "source": [
    "# Ordoptælling (vectorizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "formed-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(reddit_df['comment_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "processed-pulse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3428"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-jimmy",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "lined-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3428"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Countvectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "supported-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "random-cinema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000kr</th>\n",
       "      <th>01</th>\n",
       "      <th>019d907422_01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>...</th>\n",
       "      <th>øver</th>\n",
       "      <th>øverligt</th>\n",
       "      <th>øverste</th>\n",
       "      <th>øvet</th>\n",
       "      <th>øvetimer</th>\n",
       "      <th>øvre</th>\n",
       "      <th>øvrige</th>\n",
       "      <th>øvrigt</th>\n",
       "      <th>überdurchschnittlich</th>\n",
       "      <th>ﾟヮﾟ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000kr  01  019d907422_01  02  03  04  05  06  ...  øver  øverligt  \\\n",
       "0   0    0      0   0              0   0   0   0   0   0  ...     0         0   \n",
       "1   0    0      0   0              0   0   0   0   0   0  ...     0         0   \n",
       "2   0    0      0   0              0   0   0   0   0   0  ...     0         0   \n",
       "3   0    0      0   0              0   0   0   0   0   0  ...     0         0   \n",
       "4   0    0      0   0              0   0   0   0   0   0  ...     0         0   \n",
       "\n",
       "   øverste  øvet  øvetimer  øvre  øvrige  øvrigt  überdurchschnittlich  ﾟヮﾟ  \n",
       "0        0     0         0     0       0       0                     0    0  \n",
       "1        0     0         0     0       0       0                     0    0  \n",
       "2        0     0         0     0       0       0                     0    0  \n",
       "3        0     0         0     0       0       0                     0    0  \n",
       "4        0     0         0     0       0       0                     0    0  \n",
       "\n",
       "[5 rows x 14415 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "express-brake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[deleted]    67.000000\n",
       "god          54.742846\n",
       "al           49.351776\n",
       "folk         40.191250\n",
       "år           36.689772\n",
       "stor         35.835614\n",
       "&gt          33.150415\n",
       "tak          31.653772\n",
       "Danmark      26.753062\n",
       "kommentar    26.515093\n",
       "samme        26.307562\n",
       "gang         25.675026\n",
       "lille        25.382263\n",
       "dansk        24.381739\n",
       "dag          23.914208\n",
       "spørgsmål    23.835020\n",
       "problem      22.978650\n",
       "tid          22.112148\n",
       "ting         22.009872\n",
       "penge        21.293497\n",
       "dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = df.sum()\n",
    "word_count.sort_values(ascending = False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-drill",
   "metadata": {},
   "source": [
    "### Med stopord og dokumentgrænser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "altered-webmaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "så       1320\n",
       "kan       933\n",
       "the       462\n",
       "to        415\n",
       "https     386\n",
       "bare      366\n",
       "ved       357\n",
       "nok       279\n",
       "mere      263\n",
       "godt      260\n",
       "gt        257\n",
       "lige      250\n",
       "lidt      239\n",
       "and       233\n",
       "se        223\n",
       "com       220\n",
       "www       215\n",
       "folk      207\n",
       "år        206\n",
       "tror      197\n",
       "dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('danish')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stops, max_df = 0.9)\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "word_count = df.sum()\n",
    "word_count.sort_values(ascending = False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-seeking",
   "metadata": {},
   "source": [
    "## Alternative vægtning - Tf-idfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fiscal-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "så         3115.334699\n",
       "kan        2435.694572\n",
       "the        1808.396103\n",
       "to         1510.087783\n",
       "https      1410.917629\n",
       "bare       1222.780855\n",
       "ved        1219.668670\n",
       "and        1030.215344\n",
       "nok        1009.579087\n",
       "gt          996.474777\n",
       "mere        977.064445\n",
       "godt        956.913994\n",
       "lige        928.768484\n",
       "com         918.979451\n",
       "www         904.193254\n",
       "lidt        900.883236\n",
       "0a          889.633384\n",
       "se          855.588347\n",
       "of          830.092618\n",
       "år          828.538209\n",
       "folk        824.060511\n",
       "tror        784.250825\n",
       "fordi       772.838287\n",
       "andre       769.439345\n",
       "helt        761.063850\n",
       "it          750.282866\n",
       "ret         723.188197\n",
       "få          711.348774\n",
       "danmark     702.441635\n",
       "må          689.234588\n",
       "you         687.544244\n",
       "that        679.230748\n",
       "kommer      675.278304\n",
       "ja          673.972109\n",
       "in          629.751896\n",
       "dk          624.937200\n",
       "kun         622.438705\n",
       "får         615.064459\n",
       "gør         614.010303\n",
       "nogen       610.965999\n",
       "samme       604.525528\n",
       "is          599.507662\n",
       "mener       576.901141\n",
       "uden        572.383452\n",
       "ser         569.120502\n",
       "gøre        565.829187\n",
       "synes       545.112557\n",
       "måske       538.341608\n",
       "vel         532.713598\n",
       "ting        523.095582\n",
       "dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('danish')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stops, max_df = 0.9, norm=False)\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "word_tfidfsum = df.sum()\n",
    "word_tfidfsum.sort_values(ascending = False)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-stanley",
   "metadata": {},
   "source": [
    "## Tf-idf på eksisterende tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cellular-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "reddit_df_tokenized = pd.read_csv(\"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark-comments_01032021-08032021_long_tokenized.csv\")\n",
    "reddit_df_tokenized['tokens'] = reddit_df_tokenized['tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "applicable-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_tokens = list(reddit_df_tokenized['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "tough-monitoring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[deleted]    67.000000\n",
       "god          54.742846\n",
       "al           49.351776\n",
       "folk         40.191250\n",
       "år           36.689772\n",
       "stor         35.835614\n",
       "&gt          33.150415\n",
       "tak          31.653772\n",
       "Danmark      26.753062\n",
       "kommentar    26.515093\n",
       "samme        26.307562\n",
       "gang         25.675026\n",
       "lille        25.382263\n",
       "dansk        24.381739\n",
       "dag          23.914208\n",
       "spørgsmål    23.835020\n",
       "problem      22.978650\n",
       "tid          22.112148\n",
       "ting         22.009872\n",
       "penge        21.293497\n",
       "land         21.116093\n",
       "menneske     20.414228\n",
       "hel          20.390628\n",
       "sted         20.021477\n",
       "barn         19.930291\n",
       "enig         19.403917\n",
       "ny           18.676445\n",
       "vaccine      17.294519\n",
       "måde         16.827851\n",
       "album        16.783989\n",
       "That         16.760154\n",
       "del          16.592103\n",
       "person       15.291479\n",
       "svær         14.609716\n",
       "ulovlig      14.495890\n",
       "besked       14.481778\n",
       "rigtig       14.347606\n",
       "side         14.206807\n",
       "krone        13.772239\n",
       "arbejde      13.616934\n",
       "sikker       13.588716\n",
       "indhold      13.400862\n",
       "egen         13.340181\n",
       "parti        13.083088\n",
       "forælder     12.907286\n",
       "hvid         12.689822\n",
       "Thank        12.664326\n",
       "gammel       12.541814\n",
       "forhold      12.328521\n",
       "reklame      12.304496\n",
       "dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def return_tokens(tokens):\n",
    "    return tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=return_tokens,\n",
    "    preprocessor=return_tokens,\n",
    "    token_pattern=None)\n",
    "\n",
    "transformed_documents = vectorizer.fit_transform(comments_tokens)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "word_tfidfsum = df.sum().sort_values(ascending = False)\n",
    "word_tfidfsum[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-attention",
   "metadata": {},
   "source": [
    "# Forberedelse af tekst til random forests model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-engineering",
   "metadata": {},
   "source": [
    "## Fra score til dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "bearing-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_rf = reddit_df_tokenized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "native-peter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: comment_score, dtype: int64"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df_rf['comment_score'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ethical-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_rf['upvoted'] = reddit_df_rf['comment_score'] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "demographic-stone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: upvoted, dtype: bool"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df_rf['upvoted'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "satisfactory-norway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1872\n",
       "False    1556\n",
       "Name: upvoted, dtype: int64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df_rf['upvoted'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-squad",
   "metadata": {},
   "source": [
    "## Fra tekst til dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "convinced-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(word_tfidfsum.index[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "accessible-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in top_words:\n",
    "    colname = \"token_{}\".format(word)\n",
    "    reddit_df_rf[colname] = reddit_df_rf['tokens'].apply(lambda tokens: int(word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fewer-underground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_[deleted]',\n",
       " 'token_god',\n",
       " 'token_al',\n",
       " 'token_folk',\n",
       " 'token_år',\n",
       " 'token_stor',\n",
       " 'token_&gt',\n",
       " 'token_tak',\n",
       " 'token_Danmark',\n",
       " 'token_kommentar',\n",
       " 'token_samme',\n",
       " 'token_gang',\n",
       " 'token_lille',\n",
       " 'token_dansk',\n",
       " 'token_dag',\n",
       " 'token_spørgsmål',\n",
       " 'token_problem',\n",
       " 'token_tid',\n",
       " 'token_ting',\n",
       " 'token_penge',\n",
       " 'token_land',\n",
       " 'token_menneske',\n",
       " 'token_hel',\n",
       " 'token_sted',\n",
       " 'token_barn',\n",
       " 'token_enig',\n",
       " 'token_ny',\n",
       " 'token_vaccine',\n",
       " 'token_måde',\n",
       " 'token_album',\n",
       " 'token_That',\n",
       " 'token_del',\n",
       " 'token_person',\n",
       " 'token_svær',\n",
       " 'token_ulovlig',\n",
       " 'token_besked',\n",
       " 'token_rigtig',\n",
       " 'token_side',\n",
       " 'token_krone',\n",
       " 'token_arbejde',\n",
       " 'token_sikker',\n",
       " 'token_indhold',\n",
       " 'token_egen',\n",
       " 'token_parti',\n",
       " 'token_forælder',\n",
       " 'token_hvid',\n",
       " 'token_Thank',\n",
       " 'token_gammel',\n",
       " 'token_forhold',\n",
       " 'token_reklame']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import compress\n",
    "\n",
    "list(compress(reddit_df_textdummies.columns, reddit_df_textdummies.columns.str.startswith('token_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "joint-philip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_[deleted]',\n",
       " 'token_god',\n",
       " 'token_al',\n",
       " 'token_folk',\n",
       " 'token_år',\n",
       " 'token_stor',\n",
       " 'token_&gt',\n",
       " 'token_tak',\n",
       " 'token_Danmark',\n",
       " 'token_kommentar',\n",
       " 'token_samme',\n",
       " 'token_gang',\n",
       " 'token_lille',\n",
       " 'token_dansk',\n",
       " 'token_dag',\n",
       " 'token_spørgsmål',\n",
       " 'token_problem',\n",
       " 'token_tid',\n",
       " 'token_ting',\n",
       " 'token_penge',\n",
       " 'token_land',\n",
       " 'token_menneske',\n",
       " 'token_hel',\n",
       " 'token_sted',\n",
       " 'token_barn',\n",
       " 'token_enig',\n",
       " 'token_ny',\n",
       " 'token_vaccine',\n",
       " 'token_måde',\n",
       " 'token_album',\n",
       " 'token_That',\n",
       " 'token_del',\n",
       " 'token_person',\n",
       " 'token_svær',\n",
       " 'token_ulovlig',\n",
       " 'token_besked',\n",
       " 'token_rigtig',\n",
       " 'token_side',\n",
       " 'token_krone',\n",
       " 'token_arbejde',\n",
       " 'token_sikker',\n",
       " 'token_indhold',\n",
       " 'token_egen',\n",
       " 'token_parti',\n",
       " 'token_forælder',\n",
       " 'token_hvid',\n",
       " 'token_Thank',\n",
       " 'token_gammel',\n",
       " 'token_forhold',\n",
       " 'token_reklame']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[column for column in reddit_df_textdummies.columns if column.startswith('token_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-synthesis",
   "metadata": {},
   "source": [
    "# Fra rå API-data til tabeldata (supplerende)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-vegetarian",
   "metadata": {},
   "source": [
    "## Rester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_tuples = list(zip(vectorizer.get_feature_names(), transformed_documents_as_array[0]))\n",
    "one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).reset_index(drop=True)\n",
    "\n",
    "one_doc_as_df.loc[one_doc_as_df['score']>0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
