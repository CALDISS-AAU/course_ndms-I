{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bearbejdning/\"pre-processing\" af tekst (tokenization)\n",
    "\n",
    "At arbejde med hele rå strings er for det meste ikke særlig praktisk.\n",
    "\n",
    "- Søgning på substrings matcher også, hvis det indgår inde i et ord (medmindre man bruger regular expression, som vi ikke kommer ind på her)\n",
    "- Vanskeligt at identificere nøgleord / de mest brugte eller væsentligste ord i teksten\n",
    "- Python adskiller mellem små og store bogstaver\n",
    "- Vanskeligt at identificere ord i kontekst\n",
    "\n",
    "Måden man løser mange disse udfordringer, er ved at behandle ordene i teksterne enkeltvis. På den måde gøres hvert enkelt ord til en analyseenhed. Processen i at omdanne tekst til enkeltord kaldes \"tokenization\", da de færdigbehandlede ord referes til som tokens. \n",
    "\n",
    "## Tokenization af tekst\n",
    "\n",
    "Formålet med tokenization er at konvertere tekst til enkelte analyseenheder. I denne proces frasorteres også elementer i teksten, som umiddelbart ikke bidrager med noget meningsfuldt i en analyse af indholdet (fx tegnsætning, hvorvidt ordet er stavet med stort, fyldord som fx stedord osv.)\n",
    "\n",
    "Tokenization indeholder typisk følgende skridt:\n",
    "\n",
    "1. Opdeling af tekst i enkeltord\n",
    "2. Frasortering af tegnsætning\n",
    "3. Evt. konverter til små bogstaver\n",
    "4. Frasorter stopord\n",
    "5. Evt. konverter ord til deres stamme eller navneform\n",
    "\n",
    "### Tokenization (med basisfunktioner)\n",
    "\n",
    "Lad os først se på, hvordan vi kan implementere disse skridt enkeltvis med basisfunktioner for at se, hvordan teksten ændres skridt for skridt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweetdata_url = \"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/poltweets_sample.csv\"\n",
    "tweets_df = pd.read_csv(tweetdata_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Opdeling af tekst i enkeltord\n",
    "\n",
    "Første skridt i tokenization er at omdanne teksten til en samling af enkeltord. På den måde er hver enkelt ord afgrænset, og vi kan holde styr på deres placering i teksten gennem deres indeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hjemmeværnet, Politikadetterne og Forsvaret klarer ærterne ved grænserne, så Politiet kan kaste sig over at gøre danskere mere trygge. Sikker på,at @ClausOxfeldt bliver lykkelig for vort krav. Han har jo brugt megen energi på at give DFs grænseindsats skylden for mandskabsmangel. https://t.co/PgenOdeSwn\n",
      "\n",
      "\n",
      "['Hjemmeværnet,', 'Politikadetterne', 'og', 'Forsvaret', 'klarer', 'ærterne', 'ved', 'grænserne,', 'så', 'Politiet', 'kan', 'kaste', 'sig', 'over', 'at', 'gøre', 'danskere', 'mere', 'trygge.', 'Sikker', 'på,at', '@ClausOxfeldt', 'bliver', 'lykkelig', 'for', 'vort', 'krav.', 'Han', 'har', 'jo', 'brugt', 'megen', 'energi', 'på', 'at', 'give', 'DFs', 'grænseindsats', 'skylden', 'for', 'mandskabsmangel.', 'https://t.co/PgenOdeSwn']\n"
     ]
    }
   ],
   "source": [
    "tweet = tweets_df.loc[584, 'full_text']\n",
    "words = tweet.split(\" \")\n",
    "\n",
    "print(tweet)\n",
    "print(\"\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Frasortering af tegnsætning\n",
    "\n",
    "Tegnsætning er typisk \"støj\" i text mining teknikker, så det frasorteres typisk.\n",
    "\n",
    "Her frasorteres tegnsætning ved at gå igennem hvert ord med et for loop og derefter gå igennem forskellige tegnsætninger (`punct_list`) i et for loop og erstatte dem med ingenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hjemmeværnet', 'Politikadetterne', 'og', 'Forsvaret', 'klarer', 'ærterne', 'ved', 'grænserne', 'så', 'Politiet', 'kan', 'kaste', 'sig', 'over', 'at', 'gøre', 'danskere', 'mere', 'trygge', 'Sikker', 'påat', '@ClausOxfeldt', 'bliver', 'lykkelig', 'for', 'vort', 'krav', 'Han', 'har', 'jo', 'brugt', 'megen', 'energi', 'på', 'at', 'give', 'DFs', 'grænseindsats', 'skylden', 'for', 'mandskabsmangel', 'https://tco/PgenOdeSwn']\n"
     ]
    }
   ],
   "source": [
    "punct_list = [',', '-','.','?','!']\n",
    "\n",
    "words_nopunct = []\n",
    "\n",
    "for word in words:\n",
    "    for punct in punct_list:\n",
    "        word = word.replace(punct, \"\")\n",
    "    words_nopunct.append(word)\n",
    "    \n",
    "words_nopunct = list(filter(None, words_nopunct))\n",
    "\n",
    "print(words_nopunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evt. konverter til små bogstaver\n",
    "\n",
    "Et primært formål med tokenization er, at ord med samme semantiske betydning bliver identiske. Derfor konverteres ord typisk til små bogstaver for at ensrette tekstmaterialet.\n",
    "\n",
    "I nogen tilfælde kan det dog give mening at gøre dette med det samme, så man nemmere kan identifcere egenavne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hjemmeværnet', 'politikadetterne', 'og', 'forsvaret', 'klarer', 'ærterne', 'ved', 'grænserne', 'så', 'politiet', 'kan', 'kaste', 'sig', 'over', 'at', 'gøre', 'danskere', 'mere', 'trygge', 'sikker', 'påat', '@clausoxfeldt', 'bliver', 'lykkelig', 'for', 'vort', 'krav', 'han', 'har', 'jo', 'brugt', 'megen', 'energi', 'på', 'at', 'give', 'dfs', 'grænseindsats', 'skylden', 'for', 'mandskabsmangel', 'https://tco/pgenodeswn']\n"
     ]
    }
   ],
   "source": [
    "words_lower = [word.lower() for word in words_nopunct]\n",
    "\n",
    "print(words_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Frasorter stopord\n",
    "\n",
    "Et andet formål med tokenization er, at vi står tilbage med analyseenheder (ord), som kan hjælpe os til at sige noget om indholdet i teksten. Alle sprog har ord, som ikke bærer en speciel semantisk betydning. Det kunne fx være stedord (den, det, en, et, jeg, der osv.) og bindeord (og, eller, også, efter, før osv.).\n",
    "\n",
    "Disse ord kaldes 'stopord'.\n",
    "\n",
    "Herunder definerer vi vores egen liste af stopord og frasorterer dem med et for loop og en if-betingelse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hjemmeværnet', 'politikadetterne', 'forsvaret', 'ærterne', 'grænserne', 'politiet', 'danskere', 'trygge', 'sikker', 'påat', '@clausoxfeldt', 'lykkelig', 'krav', 'energi', 'dfs', 'grænseindsats', 'skylden', 'mandskabsmangel', 'https://tco/pgenodeswn']\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['og', 'klarer', 'ved', 'så', 'kan', 'kaste', 'sig', 'over', 'at', 'gøre', 'mere', 'bliver', \n",
    "             'for', 'vort', 'han', 'har', 'jo', 'brugt', 'megen', 'på', 'give', 'for']\n",
    "\n",
    "words_nostop = []\n",
    "\n",
    "for word in words_lower:\n",
    "    if word not in stopwords:\n",
    "        words_nostop.append(word)\n",
    "        \n",
    "print(words_nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udover stopord har vi også noget \"twitter-støj\"; fx links og brug af @ til at tagge andre brugere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hjemmeværnet', 'politikadetterne', 'forsvaret', 'ærterne', 'grænserne', 'politiet', 'danskere', 'trygge', 'sikker', 'påat', 'lykkelig', 'krav', 'energi', 'dfs', 'grænseindsats', 'skylden', 'mandskabsmangel']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for word in words_nostop:\n",
    "    if word.startswith(\"@\") or word.startswith(\"https\"):\n",
    "        continue\n",
    "    tokens.append(word)\n",
    "    \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evt. konverter ord til deres stamme eller navneform\n",
    "\n",
    "Formålet med tokenization er både at frasortere støj i teksten og ensrette tokens sådan, at der kun er ét token for alle ord med samme semantiske betydning.\n",
    "\n",
    "Fx er det uhensigtsmæssigt at behandle \"Valgkamp\" og \"valgkamp\" som to forskellige tokens, da det er det samme ord bare med stort og lille forbogstav.\n",
    "\n",
    "Netop fordi hvert ord som udgangspunkt behanldes som unikt, giver det nogle udfordringer ift. grammatik, da hver ordbøjning bliver hver sit token - medmindre man gør noget ved det!\n",
    "\n",
    "Derfor er en typisk praksis enten at konvertere ordet til stammen (stemming). Her ville ord som \"koste\", \"koster\", \"kostede\" alle konverteres til \"kost\", da det er ordets stamme.\n",
    "\n",
    "En anden praksis er at konvertere til navneformen (lemmatization). Her ville ord som \"koste\", \"koster\", \"kostede\" alle konverteres til \"koste\", da det er navneform for ordet.\n",
    "\n",
    "Sådanne konverteringer kræver, at man bruger i forvejen trænede sprogmodeller, da stamme og navneform vil variere fra sprog til sprog. Heldigvis er der udviklet flere af sådanne værktøjer.\n",
    "\n",
    "Dog er sådan nogen værktøjer ikke uden fejl, da det tager lang tid for en computer at lære, hvad forskellen er på \"kost\" (noget man spiser), \"kost\" (noget man fejer med) og \"kost\" (stammen af verbum \"koste\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization som funktion\n",
    "\n",
    "Det kan være en fordel at sammensætte ens pre-processing i en funktion. På den måde bliver den nemmere at genanvende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_basic(text):\n",
    "    punct_list = [',', '-','.','?','!']\n",
    "    stopwords = ['og', 'klarer', 'ved', 'så', 'kan', 'kaste', 'sig', 'over', 'at', 'gøre', 'mere', 'bliver', \n",
    "                 'for', 'vort', 'han', 'har', 'jo', 'brugt', 'megen', 'på', 'give', 'for']\n",
    "\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.startswith(\"@\") or word.startswith(\"https\"):\n",
    "            continue\n",
    "\n",
    "        for punct in punct_list:\n",
    "            word = word.replace(punct, \"\")\n",
    "\n",
    "        word = word.lower()\n",
    "\n",
    "        if word not in stopwords:\n",
    "            tokens.append(word)\n",
    "\n",
    "\n",
    "    tokens = list(filter(None, tokens))\n",
    "\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hjemmeværnet, Politikadetterne og Forsvaret klarer ærterne ved grænserne, så Politiet kan kaste sig over at gøre danskere mere trygge. Sikker på,at @ClausOxfeldt bliver lykkelig for vort krav. Han har jo brugt megen energi på at give DFs grænseindsats skylden for mandskabsmangel. https://t.co/PgenOdeSwn\n",
      "\n",
      "\n",
      "['hjemmeværnet', 'politikadetterne', 'forsvaret', 'ærterne', 'grænserne', 'politiet', 'danskere', 'trygge', 'sikker', 'påat', 'lykkelig', 'krav', 'energi', 'dfs', 'grænseindsats', 'skylden', 'mandskabsmangel']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_basic(tweet)\n",
    "\n",
    "print(tweet)\n",
    "print(\"\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens som pandas series\n",
    "\n",
    "Ovenstående funktion returnerer en liste af tokens. Lister kan nemt konverteres til en pandas series. På den måde kan man bruge pandas metoder til at arbejde med tekst-værdierne.\n",
    "\n",
    "I nedenstående laves en hurtig ordoptælling. Det giver selvfølgelig ikke meget mening for så kort en tekst, men ville også fungere på en langt længere tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hjemmeværnet        1\n",
       "påat                1\n",
       "skylden             1\n",
       "grænseindsats       1\n",
       "dfs                 1\n",
       "energi              1\n",
       "krav                1\n",
       "lykkelig            1\n",
       "sikker              1\n",
       "politikadetterne    1\n",
       "trygge              1\n",
       "danskere            1\n",
       "politiet            1\n",
       "grænserne           1\n",
       "ærterne             1\n",
       "forsvaret           1\n",
       "mandskabsmangel     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_series = pd.Series(tokens)\n",
    "\n",
    "tokens_series.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
