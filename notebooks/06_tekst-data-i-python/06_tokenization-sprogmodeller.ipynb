{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization med sprogmodeller\n",
    "\n",
    "Fordelen ved at bruge sprogmodeller er, at metoder til at tokenize er indbygget. Derudover kan vi nemt få lemma af vores tokens, da sprogmodellen netop kender sproget, og derfor kan gradbøje ordene i teksten.\n",
    "\n",
    "Sprogmodeller hjælper os også til at finde de mere meningsfulde ord. Visse ordklasser (part-of-speech) er ofte \"støj\" fra et tekstanalyse-perspektiv; fx hjælpeord (AUX), stedord (PRON), forholdsord (ADP) og bindeord (CONJ).\n",
    "\n",
    "Med en sprogmodel kan vi blot udvælge de ordtyper, som vi vil beholde.\n",
    "\n",
    "I det nedenstående vises en alternativ måde at tokenize med brug af en dansk sprogmodel fra `spacy`. Denne indeholder desuden en prædefineret stopordsliste, som vi tager i brug.\n",
    "\n",
    "Da der er tale om twitterdata, frasorteres ord, der starter med \"@\". spaCy behandler hashtags (#-tegnet) som et token, så derfor er vi nødt til at fortælle spromodellen, at den skal behandle hashtags samlet (# sammen med ordet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "tweetdata_url = \"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/poltweets_sample.csv\"\n",
    "tweets_df = pd.read_csv(tweetdata_url)\n",
    "\n",
    "tweet = tweets_df.loc[584, 'full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hjemmeværnet, Politikadetterne og Forsvaret klarer ærterne ved grænserne, så Politiet kan kaste sig over at gøre danskere mere trygge. Sikker på,at @ClausOxfeldt bliver lykkelig for vort krav. Han har jo brugt megen energi på at give DFs grænseindsats skylden for mandskabsmangel. https://t.co/PgenOdeSwn\n",
      "\n",
      "\n",
      "['Hjemmeværnet', 'Politikadetterne', 'Forsvaret', 'ært', 'grænse', 'Politiet', 'dansker', 'tryg', 'Sikker', 'lykkelig', 'krav', 'megen', 'energi', 'DFs', 'grænseindsats', 'skyld', 'mandskabsmangel', 'https://t.co/PgenOdeSwn']\n"
     ]
    }
   ],
   "source": [
    "# Definer tokenizer\n",
    "nlp = spacy.load(\"da_core_news_sm\") # Indlæser sprogmodel\n",
    "\n",
    "# Ændring af tokenizer\n",
    "re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match) # Indlæs regex fra sprogmodel\n",
    "re_token_match = f\"({re_token_match}|#\\\\w+)\" # tilføj hashtag-pattern\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match # opdater tokenizer\n",
    "\n",
    "custom_stops = [\"god\", \"al\", \"stor\", \"ny\", \"tak\", \"dag\"] # Definerer kontekstspecifikke stopord\n",
    "default_stopwords = list(nlp.Defaults.stop_words) # Indlæser prædefineret stopordsliste\n",
    "stop_words = default_stopwords + custom_stops # Danner samlet stopordsliste\n",
    "pos_tags = ['PROPN', 'ADJ', 'NOUN'] # Definerer POS-tags som skal bevares: egenavne, adjektiver og navneord\n",
    "\n",
    "doc = nlp(tweet) # Kører sprogmodel på tweet\n",
    "\n",
    "tokens = [] # Tom liste til at fylde tokens i \n",
    "\n",
    "for word in doc: # Looper igennem hvert ord i tweet\n",
    "    if word.lemma_.startswith(\"@\"): # Ord må ikke starte med @ - går videre til næste ord, hvis det gør\n",
    "        continue \n",
    "    if word.lemma_.startswith(\"#\"): # Ord må ikke starte med # - går videre til næste ord, hvis det gør\n",
    "        continue \n",
    "    if (len(word.lemma_) < 3): # Ord må ikke være mindre end 3 karakterer - går videre til næste ord, hvis det er\n",
    "        continue\n",
    "    if (word.pos_ in pos_tags) and (word.lemma_ not in stop_words): # Tjek at ordets POS-tag indgår i listen af accepterede tags og at ordet ikke er stopord\n",
    "        # Bemærk at denne betingelse nås kun for dem, som ikke opfylder betingelserne fra de andre if-linjer\n",
    "        tokens.append(word.lemma_) # Tilføj ordets lemma til tokens, hvis if-betingelse er opfyldt\n",
    "\n",
    "print(tweet) # Print oprindelige tweet\n",
    "print(\"\\n\")            \n",
    "print(tokens) # Print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer funktion med spaCy\n",
    "\n",
    "Ovenstående kan med fordel laves til en funktion, så vi nemt kan foretage samme tokenization på alle tweets i datasættet.\n",
    "\n",
    "Samtidig genindlæses sprogmodellen, hvor vi slår visse processer fra (\"ner\"). Dette gør behandlingen hurtigere, da den ellers kører igennem alle processer i sprogmodellen, uanset om disse bruges eller ej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\", disable = [\"ner\"])\n",
    "\n",
    "# Ændring af tokenizer\n",
    "re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match) # Indlæs regex fra sprogmodel\n",
    "re_token_match = f\"({re_token_match}|#\\\\w+)\" # tilføj hashtag-pattern\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match # opdater tokenizer\n",
    "\n",
    "def tokenizer_spacy(text): # Definerer funktion ud fra koden fra tidligere\n",
    "    custom_stops = [\"god\", \"al\", \"stor\", \"ny\", \"tak\", \"dag\"] # Definerer kontekstspecifikke stopord\n",
    "    default_stopwords = list(nlp.Defaults.stop_words) # Indlæser prædefineret stopordsliste\n",
    "    stop_words = default_stopwords + custom_stops # Danner samlet stopordsliste\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN'] # Definerer POS-tags som skal bevares: egenavne, adjektiver og navneord\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc: # Looper igennem hvert ord i tweet\n",
    "        if word.lemma_.startswith(\"@\"): # Ord må ikke starte med @ - går videre til næste ord, hvis det gør\n",
    "            continue \n",
    "        if word.lemma_.startswith(\"#\"): # Ord må ikke starte med # - går videre til næste ord, hvis det gør\n",
    "            continue \n",
    "        if (len(word.lemma_) < 3): # Ord må ikke være mindre end 3 karakterer - går videre til næste ord, hvis det er\n",
    "            continue\n",
    "        if (word.pos_ in pos_tags) and (word.lemma_ not in stop_words): # Tjek at ordets POS-tag indgår i listen af accepterede tags og at ordet ikke er stopord\n",
    "            # Bemærk at denne betingelse nås kun for dem, som ikke opfylder betingelserne fra de andre if-linjer\n",
    "            tokens.append(word.lemma_) # Tilføj ordets lemma til tokens, hvis if-betingelse er opfyldt\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen kan nu bruges på tweets (eller andre strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hjemmeværnet, Politikadetterne og Forsvaret klarer ærterne ved grænserne, så Politiet kan kaste sig over at gøre danskere mere trygge. Sikker på,at @ClausOxfeldt bliver lykkelig for vort krav. Han har jo brugt megen energi på at give DFs grænseindsats skylden for mandskabsmangel. https://t.co/PgenOdeSwn\n",
      "\n",
      "\n",
      "['Hjemmeværnet', 'Politikadetterne', 'Forsvaret', 'ært', 'grænse', 'Politiet', 'dansker', 'tryg', 'Sikker', 'lykkelig', 'krav', 'megen', 'energi', 'DFs', 'grænseindsats', 'skyld', 'mandskabsmangel', 'https://t.co/PgenOdeSwn']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_spacy(tweet)\n",
    "\n",
    "print(tweet)\n",
    "print(\"\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens kan nemt optælles ved at konvertere dem til en pandas series og bruge `.value_counts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hjemmeværnet               1\n",
       "Politikadetterne           1\n",
       "mandskabsmangel            1\n",
       "skyld                      1\n",
       "grænseindsats              1\n",
       "DFs                        1\n",
       "energi                     1\n",
       "megen                      1\n",
       "krav                       1\n",
       "lykkelig                   1\n",
       "Sikker                     1\n",
       "tryg                       1\n",
       "dansker                    1\n",
       "Politiet                   1\n",
       "grænse                     1\n",
       "ært                        1\n",
       "Forsvaret                  1\n",
       "https://t.co/PgenOdeSwn    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_series = pd.Series(tokens)\n",
    "tokens_series.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
