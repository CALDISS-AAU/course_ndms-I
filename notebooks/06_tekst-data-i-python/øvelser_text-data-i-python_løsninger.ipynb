{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a20338-18a8-4dfa-8fde-a2fc7e8e0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indlæser data brugt i øvelserne\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "reddit_df = pd.read_csv(\"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark-comments_01032021-08032021_long.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedb98e-37a2-40b4-aed3-3feaa40bef4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## ØVELSE 1: Sentiment Analysis\n",
    "\n",
    "Foretag sentiment analysis enten på eget data eller på r/Denmark data: [reddit_rdenmark-comments_01032021-08032021_long.csv](https://raw.githubusercontent.com/CALDISS-AAU/course_ddf/master/datasets/reddit_rdenmark-comments_01032021-08032021_long.csv)\n",
    "\n",
    "(DaCy/senda for dansk, TextBlob for engelsk)\n",
    "\n",
    "**Bemærk:** Sentiment analysis med især DaCy kan tage lang tid på større datasæt. I kan med fordel arbejde med et subset i denne øvelse, for at teste funktionerne af.\n",
    "\n",
    "1. Anvend sentiment analysis på enkelte tekststykker\n",
    "2. Anvend sentiment analysis på et subset (enten med egne betingelser eller med [`pd.sample`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html))\n",
    "3. Foretag opsummering af subsettet, der giver indikation af, hvorvidt tonen i materialet er overvejende positiv eller negativ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e503b15-4d71-47cb-8c67-68b7a1953a83",
   "metadata": {},
   "source": [
    "### (En) løsning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00b6977-00a6-462f-8aba-6a87f62f4621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "/opt/tljh/user/lib/python3.9/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'da_dacy_medium_tft' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/tljh/user/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.da.Danish at 0x7f1bcd5215b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indlæser DaCy\n",
    "import dacy\n",
    "from dacy.sentiment import add_senda\n",
    "\n",
    "# Indlæser sprogmodel\n",
    "nlp = dacy.load(\"da_dacy_medium_tft-0.0.0\")\n",
    "\n",
    "# Tilføjer senda sentiment anlaysis moel til pipeline\n",
    "add_senda(nlp, force_extension = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa6a0d9-6460-439b-afb0-81a6a483c3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Du synes ikke det er forståeligt at folk er chokerede over en prisstigning fra 309kr til 419kr på sportspakken? Det er en stigning på 35%. Det er fuldstændig vanvittigt for en tjeneste der er rigeligt dyr i forvejen.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Udvælger tilfældig kommentar\n",
    "\n",
    "a_comment = reddit_df.loc[376, 'comment_body']\n",
    "a_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eaa91a8-d885-4f15-9774-fa51e776366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Bruger sentiment analysis på kommentar:\n",
    "\n",
    "doc = nlp(a_comment)\n",
    "\n",
    "print(doc._.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5009f6-5152-4eff-9a93-8b5cb5b236ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner sample med 15 kommentarer med pd.sample\n",
    "\n",
    "reddit_sample = reddit_df.sample(n = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd27ce88-ad52-4ae6-b66b-5b7c4d1e981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner wrapper funktion\n",
    "\n",
    "def simple_sentiment(text):\n",
    "    doc = nlp(text)\n",
    "    polarity = doc._.polarity\n",
    "    \n",
    "    return(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbbde0f-759b-4a04-9ede-89e872dead2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bruger wrapper på sample\n",
    "\n",
    "reddit_sample['sentiment'] = reddit_sample['comment_body'].apply(simple_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569e889b-bfc8-4e82-93dd-16c84fdd49f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    10\n",
       "neutral      3\n",
       "positive     2\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opsummering/optælling\n",
    "\n",
    "reddit_sample['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026e1ec-c6fa-4d85-bfec-9c4c434935f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## ØVELSE 2: Simpel teksthåndtering\n",
    "\n",
    "I øvelserne i dag skal i arbejde med et datasæt bestående af kommentarer fra reddit. Alle kommentarer er taget fra posts på r/denmark (reddit.com/r/denmark) fra 1/3-8/3 2021.\n",
    "\n",
    "1. Indlæs data som en pandas data frame\n",
    "    - Link til data: https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark-comments_01032021-08032021_long.csv\n",
    "2. Dan et subset bestående af alle kommentarer, der nævner \"menneskerettigheder\" (kommentarteksten er i kolonnen `comment_body`). Hvor mange kommentarer er der?\n",
    "\n",
    "**Bonus**\n",
    "- Kan du udregne gennemsnitsscore for de kommentarer, der nævner menneskerettigheder? (score fremgår af kolonnen `comment_score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafbcff-d331-473d-8a29-f61089cd028b",
   "metadata": {},
   "source": [
    "### (En) løsning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c37056-dfb3-4cd6-ac6f-3358609f30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset med alle kommentarerne \"menneskerettigheder\"\n",
    "\n",
    "reddit_subset = reddit_df.loc[reddit_df['comment_body'].str.contains(\"menneskerettigheder\"), :]\n",
    "\n",
    "# Alternativ - både med lille og stort for bogstav\n",
    "reddit_subset = reddit_df.loc[reddit_df['comment_body'].str.contains(\"menneskerettigheder\") | reddit_df['comment_body'].str.contains(\"Menneskerettigheder\"), :]\n",
    "\n",
    "# Alternativ - omdanner til lower-case først\n",
    "reddit_df['comment_body_l'] = reddit_df['comment_body'].str.lower()\n",
    "reddit_subset = reddit_df.loc[reddit_df['comment_body'].str.contains(\"menneskerettigheder\"), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51ab676-3864-4706-95bc-fd6ec8fab3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 53)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Antal kommentarer (ud fra hvor mange rækker, der er)\n",
    "\n",
    "reddit_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "379061f3-a1f8-4776-a3ca-24f91c571f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.45454545454545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bonus: Gennemsnitsscore for kommentarerne\n",
    "\n",
    "reddit_subset['comment_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b6faf-159a-4483-bcad-e5b45ebf3e74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## ØVELSE 3: Brug af sprogmodel\n",
    "\n",
    "1. Udvælg en enkelt kommentar fra reddit datasættet. Kommentarteksten findes i kolonnen \"comment_body\" (fx `comment = reddit_df.loc[200, 'comment_body']`)\n",
    "\n",
    "2. Analysér kommentaren med spaCy sprogmodellen (omdan kommentaren til et `doc` objekt). Husk at installér og indlæs sprogmodellen først:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "!python -m spacy download da_core_news_sm\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "```\n",
    "\n",
    "3. Er der named entities i kommentaren? (`doc.ents`) I så fald hvilke?\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "- Lav en liste, der kun indeholder ord fra kommentaren med ordklassen \"NOUN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8931288-c538-42d7-b540-b9e3587d5d93",
   "metadata": {},
   "source": [
    "### (En) løsning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b5251d3-3fef-48a4-b2dd-412ace99d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indlæser spacy og dansk sprogmodel\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c926ebdc-c49b-4561-b2cd-b0796bcf654d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jeg tænker EB gør det, fordi at der er et hårdt rygte på, at Lars Løkke er ved at starte eget parti op.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Udvælg kommentar fra reddit sæt\n",
    "comment = reddit_df.loc[200, 'comment_body']\n",
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09b16f9-cec3-4048-8561-d6d13293c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brug sprogmodel på kommentar\n",
    "doc = nlp(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc6d987-bfd1-4816-856a-d0d6e729b092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Lars Løkke,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Er der named entities? (Ja)\n",
    "\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c23e29bd-5618-4571-8f20-f061f81efd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parti']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bonus: Liste kun med navneord i kommentaren\n",
    "\n",
    "nouns = [] \n",
    "\n",
    "for word in doc:\n",
    "    if word.pos_ == \"NOUN\":\n",
    "        nouns.append(word.text)\n",
    "        \n",
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2ca26-0b9a-4e22-b997-e4bbe169192a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## ØVELSE 4: Tidy text data (reddit data)\n",
    "\n",
    "Du skal nu tokenize alle *kommentarerne* i reddit datasættet (kolonnen `comment_body`)\n",
    "\n",
    "1. Brug `.apply()` til at anvende tokenizer funktion på hele reddit datasættet til at lave en tokens kolonne (det kan være en god ide at teste funktionen med en enkelt kommentar først)\n",
    "2. Brug `.explode()` til at konvertere data til et tidy format\n",
    "3. Brug `.value_counts()` til at optælle tokens\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "- Undersøg, hvor mange gange coronavirus er nævnt (tænkt gerne synonymer med!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa91d33-0cba-4c21-a097-cde800655183",
   "metadata": {},
   "source": [
    "### (En) løsning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f93e1a14-1733-4bc9-b800-250fb23aa2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpel tokenizer funktion (kopieret fra notebook 07)\n",
    "from spacy.lang.da import Danish\n",
    "nlp = Danish() # Indlæser \"tom\" sprogmodel\n",
    "tokenizer = nlp.tokenizer # henter tokenizer\n",
    "\n",
    "def tokenizer_simple(text):\n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "    \n",
    "    doc = tokenizer(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for word in doc:\n",
    "        if len(word.text) < 3:\n",
    "            continue\n",
    "        if word.text not in stop_words:\n",
    "            tokens.append(word.text)\n",
    "    \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25aabd42-5645-4495-9e45-aec12819e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize alle kommentarer - dan variabel med tokens\n",
    "reddit_df['tokens'] = reddit_df['comment_body'].apply(tokenizer_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e705ddd5-208b-4078-b13b-b6f5aac8c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omdan til tidy format (en række per ord)\n",
    "\n",
    "reddit_df = reddit_df.explode('tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a73b396-ae2b-4c0e-8f52-e6f0f2657c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Det            955\n",
       "Jeg            716\n",
       "the            414\n",
       "bare           345\n",
       "godt           251\n",
       "              ... \n",
       "Seier            1\n",
       "uvidst           1\n",
       "Retorikken       1\n",
       "strømninger      1\n",
       "pubeshår         1\n",
       "Name: tokens, Length: 15571, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optæl tokens\n",
    "reddit_df['tokens'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c15f4bd8-93d4-434a-81df-a8d5b33a2662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 54)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bonus: hvor mange gange er coronavirus eller lignende nævnt? (65 gange baseret på antal rækker efter filtrering på nøgleord)\n",
    "\n",
    "coronawords = ['corona', 'coronavirus', 'covid', 'covid-19', 'covid19']\n",
    "\n",
    "reddit_df.loc[reddit_df['tokens'].str.lower().isin(coronawords), :].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
