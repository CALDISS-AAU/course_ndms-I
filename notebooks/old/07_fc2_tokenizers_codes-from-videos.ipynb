{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koder brugt i video 1-6 til Flipped Classroom 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stanza - med det hele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pakker\n",
    "\n",
    "import stanza\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download ressourcer\n",
    "#nltk.download('stopwords')\n",
    "#stanza.download('da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indlæs data\n",
    "\n",
    "redditdata_url = \"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark_q=danmark_01012020-15032021_long.zip\"\n",
    "reddit_df = pd.read_csv(redditdata_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "filter_start = int(datetime(2020,1,1,0,0).timestamp())\n",
    "filter_end = int(datetime(2020,7,1,0,0).timestamp())\n",
    "\n",
    "reddit_df = reddit_df.loc[reddit_df['post_num_comments'].astype(int) > 5, :]\n",
    "reddit_df = reddit_df.loc[(reddit_df['post_created_utc'] >= filter_start) & (reddit_df['post_created_utc'] < filter_end), :]\n",
    "reddit_df = reddit_df.loc[reddit_df['comment_body'].str.len() > 30, :]\n",
    "\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer tokenizer\n",
    "\n",
    "nlp = stanza.Pipeline('da')\n",
    "\n",
    "def tokenizer_stanza(text): # Definerer funktion ud fra koden fra tidligere    \n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.lemma) < 2):\n",
    "                continue\n",
    "            if (word.pos in pos_tags) and (word.lemma not in stop_words):\n",
    "                tokens.append(word.lemma)\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stanza - kun tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer tokenizer\n",
    "\n",
    "nlp = stanza.Pipeline('da', processors = 'tokenize')\n",
    "\n",
    "def tokenizer_stanza_simple(text): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.text) < 2):\n",
    "                continue\n",
    "            if word.text.lower() not in stop_words:\n",
    "                tokens.append(word.text.lower())\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy - Kun tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#!python -m spacy download da_core_news_sm # download sprogmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jer',\n",
       " 'efter',\n",
       " 'hvorefter',\n",
       " 'gør',\n",
       " 'være',\n",
       " 'her',\n",
       " 'for',\n",
       " 'lavet',\n",
       " 'vi',\n",
       " 'ved',\n",
       " 'derfra',\n",
       " 'men',\n",
       " 'vær',\n",
       " 'det',\n",
       " 'ham',\n",
       " 'ses',\n",
       " 'samme',\n",
       " 'blive',\n",
       " 'sammen',\n",
       " 'hvornår',\n",
       " 'ene',\n",
       " 'jo',\n",
       " 'jeg',\n",
       " 'mest',\n",
       " 'hvilken',\n",
       " 'anden',\n",
       " 'allerede',\n",
       " 'skal',\n",
       " 'selv',\n",
       " 'min',\n",
       " 'mange',\n",
       " 'alle',\n",
       " 'dens',\n",
       " 'en',\n",
       " 'ingen',\n",
       " 'forrige',\n",
       " 'bliver',\n",
       " 'ligesom',\n",
       " 'under',\n",
       " 'mere',\n",
       " 'i',\n",
       " 'vil',\n",
       " 'har',\n",
       " 'lav',\n",
       " 'over',\n",
       " 'egen',\n",
       " 'tidligere',\n",
       " 'du',\n",
       " 'tilbage',\n",
       " 'måske',\n",
       " 'henover',\n",
       " 'den',\n",
       " 'dermed',\n",
       " 'hvem',\n",
       " 'havde',\n",
       " 'ny',\n",
       " 'mine',\n",
       " 'end',\n",
       " 'derpå',\n",
       " 'kom',\n",
       " 'via',\n",
       " 'hendes',\n",
       " 'dine',\n",
       " 'flere',\n",
       " 'nogensinde',\n",
       " 'jeres',\n",
       " 'ud',\n",
       " 'var',\n",
       " 'flest',\n",
       " 'kan',\n",
       " 'langs',\n",
       " 'mig',\n",
       " 'nogle',\n",
       " 'god',\n",
       " 'alligevel',\n",
       " 'kommer',\n",
       " 'derefter',\n",
       " 'derfor',\n",
       " 'og',\n",
       " 'hans',\n",
       " 'mit',\n",
       " 'herefter',\n",
       " 'derved',\n",
       " 'hver',\n",
       " 'dem',\n",
       " 'mindre',\n",
       " 'næste',\n",
       " 'lad',\n",
       " 'lidt',\n",
       " 'begge',\n",
       " 'mens',\n",
       " 'fordi',\n",
       " 'hermed',\n",
       " 'intet',\n",
       " 'gøre',\n",
       " 'de',\n",
       " 'kun',\n",
       " 'lille',\n",
       " 'nær',\n",
       " 'lige',\n",
       " 'eneste',\n",
       " 'gennem',\n",
       " 'heller',\n",
       " 'enten',\n",
       " 'hvor',\n",
       " 'således',\n",
       " 'nok',\n",
       " 'overalt',\n",
       " 'få',\n",
       " 'han',\n",
       " 'imellem',\n",
       " 'af',\n",
       " 'op',\n",
       " 'mellem',\n",
       " 'dette',\n",
       " 'længere',\n",
       " 'ned',\n",
       " 'stadig',\n",
       " 'om',\n",
       " 'fra',\n",
       " 'gjort',\n",
       " 'så',\n",
       " 'altid',\n",
       " 'hvorfor',\n",
       " 'skulle',\n",
       " 'også',\n",
       " 'siden',\n",
       " 'nu',\n",
       " 'enhver',\n",
       " 'vores',\n",
       " 'os',\n",
       " 'hende',\n",
       " 'ind',\n",
       " 'indtil',\n",
       " 'andet',\n",
       " 'lave',\n",
       " 'senere',\n",
       " 'som',\n",
       " 'hvorhen',\n",
       " 'meget',\n",
       " 'synes',\n",
       " 'været',\n",
       " 'omkring',\n",
       " 'sådan',\n",
       " 'før',\n",
       " 'herpå',\n",
       " 'igennem',\n",
       " 'have',\n",
       " 'imens',\n",
       " 'sige',\n",
       " 'vore',\n",
       " 'hvordan',\n",
       " 'alt',\n",
       " 'udover',\n",
       " 'disse',\n",
       " 'hvilkes',\n",
       " 'temmelig',\n",
       " 'uden',\n",
       " 'imod',\n",
       " 'mindst',\n",
       " 'øvrigt',\n",
       " 'andre',\n",
       " 'næsten',\n",
       " 'gørende',\n",
       " 'alene',\n",
       " 'undtagen',\n",
       " 'endnu',\n",
       " 'hel',\n",
       " 'hen',\n",
       " 'hun',\n",
       " 'fleste',\n",
       " 'nemlig',\n",
       " 'hvorimod',\n",
       " 'med',\n",
       " 'ville',\n",
       " 'selvom',\n",
       " 'blandt',\n",
       " 'ellers',\n",
       " 'denne',\n",
       " 'gjorde',\n",
       " 'bag',\n",
       " 'dog',\n",
       " 'først',\n",
       " 'heri',\n",
       " 'hvad',\n",
       " 'hvis',\n",
       " 'dig',\n",
       " 'hvorfra',\n",
       " 'ikke',\n",
       " 'et',\n",
       " 'sig',\n",
       " 'syntes',\n",
       " 'deri',\n",
       " 'på',\n",
       " 'ens',\n",
       " 'noget',\n",
       " 'din',\n",
       " 'hvorved',\n",
       " 'man',\n",
       " 'er',\n",
       " 'at',\n",
       " 'hvori',\n",
       " 'til',\n",
       " 'eller',\n",
       " 'kunne',\n",
       " 'nyt',\n",
       " 'tit',\n",
       " 'blev',\n",
       " 'deres',\n",
       " 'da',\n",
       " 'hvilke',\n",
       " 'burde',\n",
       " 'må',\n",
       " 'nogen',\n",
       " 'bør',\n",
       " 'foran',\n",
       " 'igen',\n",
       " 'aldrig',\n",
       " 'der']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "list(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer tokenizer funktion \n",
    "\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "def tokenizer_spacy_simple(text):\n",
    "    \n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "\n",
    "    doc = nlp.tokenizer(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.text) < 2):\n",
    "            continue\n",
    "        if word.text.lower() not in stop_words:\n",
    "            tokens.append(word.text.lower())\n",
    "\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_spacy_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn tokenizer (fra CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer tokenizer funktion\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "def tokenizer_sklearn(text):\n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "    \n",
    "    words = tokenizer(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (len(word) < 2):\n",
    "            continue\n",
    "        if word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sammenligning af tokenizers\n",
    "\n",
    "Nedenstående kode opretter test-funktion for hver tokenizer. Test-funktionen kører tokenizeren på 100 reddit posts og viser, hvor lang tid tokenization tager i sekunder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "\n",
    "nlp_stanza = stanza.Pipeline('da')\n",
    "nlp_stanza_simple = stanza.Pipeline('da', processors = 'tokenize')\n",
    "nlp_spacy_simple = spacy.load(\"da_core_news_sm\")\n",
    "sklearn_tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "def tokenizer_stanza(text, nlp = nlp_stanza): # Definerer funktion ud fra koden fra tidligere    \n",
    "\n",
    "    stop_words = list(nlp_spacy_simple.Defaults.stop_words)\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.lemma) < 2):\n",
    "                continue\n",
    "            if (word.pos in pos_tags) and (word.lemma not in stop_words):\n",
    "                tokens.append(word.lemma)\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "def tokenizer_stanza_simple(text, nlp = nlp_stanza_simple): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(nlp_spacy_simple.Defaults.stop_words)\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.text) < 2):\n",
    "                continue\n",
    "            if word.text.lower() not in stop_words:\n",
    "                tokens.append(word.text.lower())\n",
    "                \n",
    "    return(tokens)\n",
    "\n",
    "def tokenizer_spacy_simple(text, nlp = nlp_spacy_simple): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(nlp_spacy_simple.Defaults.stop_words)\n",
    "\n",
    "    doc = nlp.tokenizer(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.text) < 2):\n",
    "            continue\n",
    "        if word.text.lower() not in stop_words:\n",
    "            tokens.append(word.text.lower())\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "def tokenizer_sklearn(text, tokenizer = sklearn_tokenizer):\n",
    "    stop_words = list(nlp_spacy_simple.Defaults.stop_words)\n",
    "    \n",
    "    words = tokenizer(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (len(word) < 2):\n",
    "            continue\n",
    "        if word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    \n",
    "    return(tokens)\n",
    "\n",
    "def stanza_full_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza)\n",
    "    \n",
    "    print(\"stanza full: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def stanza_simple_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza_simple)\n",
    "    \n",
    "    print(\"stanza simple: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def spacy_simple_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_spacy_simple)\n",
    "    \n",
    "    print(\"spacy simple: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def sklearn_tester():\n",
    "    start_time = time.time()\n",
    "          \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_sklearn)\n",
    "    \n",
    "    print(\"sklearn: {0:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_full_tester()\n",
    "stanza_simple_tester()\n",
    "spacy_simple_tester()\n",
    "sklearn_tester()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordoptælling med vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagr kommentarer i objekt for sig\n",
    "\n",
    "comments = list(reddit_df['comment_body'])\n",
    "\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countvectorizer på kommentarer - rå tekst\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "# Konverter fittet vectorizer til array\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Konverter array til document-term matrix\n",
    "\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optælling af ord på tværs af dokumenter\n",
    "\n",
    "word_count = df.sum()\n",
    "word_count.sort_values(ascending = False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConuntVectorizer med stopord og dokumentgrænser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indlæser spacy for at bruge spacy stopordsliste\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "custom_stops = ['gt', 'bare', 'the', 'to', 'når', 'https', 'helt', 'of', 'se', 'in', 'www', 'is', 'you', 'dk', 'får', 'com', 'ret', 'it', 'that', 'år', 'siger',\n",
    "               'hele', 'går', 'ting', 'ser', 'del', 'vel', 'tage', 'set', 'are', 'be', 'not', 'but', 'amp']\n",
    "\n",
    "stops = list(nlp.Defaults.stop_words) + custom_stops\n",
    "\n",
    "# Indstiller vectorizer - stopord og maksimalt antal dokumenter, ord må indgå i (max. 70%)\n",
    "vectorizer = CountVectorizer(stop_words = stops, max_df = 0.7)\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter array til document-term matrix\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Ordoptælling\n",
    "word_count = df.sum()\n",
    "word_count.sort_values(ascending = False)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative vægtning af ord: Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-idf vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "custom_stops = ['gt', 'bare', 'the', 'to', 'når', 'https', 'helt', 'of', 'se', 'in', 'www', 'is', 'you', 'dk', 'får', 'com', 'ret', 'it', 'that', 'år', 'siger',\n",
    "               'hele', 'går', 'ting', 'ser', 'del', 'vel', 'tage', 'set', 'are', 'be', 'not', 'but', 'amp']\n",
    "\n",
    "stops = list(nlp.Defaults.stop_words) + custom_stops\n",
    "\n",
    "# Indstil tfidf vectorizer - samme indstillinger som før\n",
    "vectorizer = TfidfVectorizer(stop_words = stops, max_df = 0.7, norm = False)\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter array til document-term matrix\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Ordoptælling\n",
    "word_tfidfsum = df.sum()\n",
    "word_tfidfsum.sort_values(ascending = False)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf vectorizer på eksisterende tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion brugt til at tokenize data\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\", disable = ['parser', 'ner', 'textcat'])\n",
    "\n",
    "def tokenizer_spacy(text):\n",
    "    custom_stops = ['gt', 'bare', 'the', 'to', 'når', 'https', 'helt', 'of', 'se', 'in', 'www', 'is', 'you', 'dk', 'får', 'com', 'ret', 'it', 'that', 'år', 'siger',\n",
    "               'hele', 'går', 'ting', 'ser', 'del', 'vel', 'tage', 'set', 'are', 'be', 'not', 'but', 'amp']\n",
    "    stop_words = list(nlp.Defaults.stop_words) + custom_stops\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.lemma_) == 1):\n",
    "            continue\n",
    "        if (word.pos_ in pos_tags) and (word.lemma_.lower() not in stop_words):\n",
    "            tokens.append(word.lemma_.lower())\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['comment_tokens'] = reddit_df['comment_body'].apply(tokenizer_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner kopi af data\n",
    "\n",
    "reddit_df_tokenized = reddit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evt. indlæs allerede eksisterende tokenized data\n",
    "#import ast\n",
    "#reddit_df_tokenized = pd.read_csv(\"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark_q=danmark_01012020-30062020_long_filtered_tokenized.zip\")\n",
    "#reddit_df_tokenized['tokens'] = reddit_df_tokenized['tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "reddit_df_tokenized = reddit_df_tokenized.loc[reddit_df_tokenized['comment_tokens'].apply(lambda tokens: len(tokens) > 1), :]\n",
    "\n",
    "# Lagr kommentarer for sig\n",
    "comments_tokens = list(reddit_df_tokenized['comment_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidfvectorizer på tokens\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dummyfunktion - bruges som tokenizer-funktion i vectorizer\n",
    "def return_tokens(tokens):\n",
    "    return tokens\n",
    "\n",
    "# Indstiller vectorizer med brug af dummyfunktion (returnerer blot tokens, da data allerede er tokenized)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=return_tokens,\n",
    "    preprocessor=return_tokens,\n",
    "    token_pattern=None,\n",
    "    norm = False)\n",
    "\n",
    "# Fitter vectorizer\n",
    "transformed_documents = vectorizer.fit_transform(comments_tokens)\n",
    "\n",
    "# Konverter til array\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter til document-term matrix\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Ordoptælling\n",
    "word_tfidfsum = df.sum().sort_values(ascending = False)\n",
    "word_tfidfsum[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fra tekst til features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy-variabel for upvoted/ikke upvoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner kopi af data\n",
    "\n",
    "reddit_df_rf = reddit_df_tokenized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tjekker indhold af variabel \"comment_score\"\n",
    "reddit_df_rf['comment_score'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner dummy for upvoted\n",
    "reddit_df_rf['comment_upvoted'] = reddit_df_rf['comment_score'] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tjekker indhold af ny variabel\n",
    "reddit_df_rf['comment_upvoted'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optælling på ny variabel\n",
    "reddit_df_rf['comment_upvoted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel for downvoted\n",
    "reddit_df_rf['comment_downvoted'] = reddit_df_rf['comment_score'] < 1\n",
    "\n",
    "# Optælling\n",
    "reddit_df_rf['comment_downvoted'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra tekst til dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner ordliste af 50 mest hyppige ord baseret på tfidf fra tidligere\n",
    "top_words = list(word_tfidfsum.index[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop igennem hvert ord i topwords og dan dummyvariabel for hvorvidt ord indgår i kommentar eller ej (ud fra token-liste)\n",
    "for word in top_words:\n",
    "    colname = \"token_{}\".format(word) # Denne linje giver dummyvariabel for ord præfix \"token_\"\n",
    "    reddit_df_rf[colname] = reddit_df_rf['comment_tokens'].apply(lambda tokens: int(word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tjek dummyvariable for tekst\n",
    "[column for column in reddit_df_rf.columns if column.startswith('token_')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
