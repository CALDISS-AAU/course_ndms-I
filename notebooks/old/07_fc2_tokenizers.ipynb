{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koder brugt i video 1-6 til Flipped Classroom 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stanza - med det hele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pakker\n",
    "\n",
    "import stanza\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download ressourcer\n",
    "#nltk.download('stopwords')\n",
    "#stanza.download('da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\programs\\Anaconda3\\envs\\lda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3166: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Indl√¶s data\n",
    "\n",
    "redditdata_url = \"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark_q=danmark_01012020-15032021_long.zip\"\n",
    "reddit_df = pd.read_csv(redditdata_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27918, 52)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtrer data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "filter_start = int(datetime(2020,1,1,0,0).timestamp())\n",
    "filter_end = int(datetime(2020,7,1,0,0).timestamp())\n",
    "\n",
    "reddit_df = reddit_df.loc[reddit_df['post_num_comments'].astype(int) > 5, :]\n",
    "reddit_df = reddit_df.loc[(reddit_df['post_created_utc'] >= filter_start) & (reddit_df['post_created_utc'] < filter_end), :]\n",
    "reddit_df = reddit_df.loc[reddit_df['comment_body'].str.len() > 30, :]\n",
    "\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-18 11:07:18 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "| lemma     | ddt     |\n",
      "| depparse  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-18 11:07:18 INFO: Use device: cpu\n",
      "2021-03-18 11:07:18 INFO: Loading: tokenize\n",
      "2021-03-18 11:07:18 INFO: Loading: pos\n",
      "2021-03-18 11:07:19 INFO: Loading: lemma\n",
      "2021-03-18 11:07:19 INFO: Loading: depparse\n",
      "2021-03-18 11:07:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Definer tokenizer\n",
    "\n",
    "nlp = stanza.Pipeline('da')\n",
    "\n",
    "def tokenizer_stanza(text): # Definerer funktion ud fra koden fra tidligere    \n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.lemma) < 2):\n",
    "                continue\n",
    "            if (word.pos in pos_tags) and (word.lemma not in stop_words):\n",
    "                tokens.append(word.lemma)\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15382      [&gt, sjov, sp√∏g, sp√∏g, alvor, del, Piet, Hein]\n",
       "19594                                               [Dude]\n",
       "8085     [lillebrors, forhold, enig, kraftig, forholdsr...\n",
       "2113     [bund, artikel, sag, sj√¶lden, Danmark, sen, sa...\n",
       "16826                                   [glad, BIG, These]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stanza - kun tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-18 11:07:38 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-18 11:07:38 INFO: Use device: cpu\n",
      "2021-03-18 11:07:38 INFO: Loading: tokenize\n",
      "2021-03-18 11:07:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Definer tokenizer\n",
    "\n",
    "nlp = stanza.Pipeline('da', processors = 'tokenize')\n",
    "\n",
    "def tokenizer_stanza_simple(text): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(stopwords.words('danish'))\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.text) < 2):\n",
    "                continue\n",
    "            if word.text.lower() not in stop_words:\n",
    "                tokens.append(word.text.lower())\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15382    [&gt, ingen, virker, sjove, kun, tar, sp√∏g, sp...\n",
       "19594    [dude, you‚Äôre, such, huge, influence, and, fac...\n",
       "8085     [wow, fejrer, lige, lillebrors, √∏delagte, forh...\n",
       "2113     [l√¶ser, bunden, artiklen, s√•, sager, sj√¶ldne, ...\n",
       "16826    [glad, you, like, it, here, agree, on, your, t...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy - Kun tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#!python -m spacy download da_core_news_sm # download sprogmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer tokenizer funktion \n",
    "\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "def tokenizer_spacy_simple(text):\n",
    "    \n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "\n",
    "    doc = nlp.tokenizer(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.text) < 2):\n",
    "            continue\n",
    "        if word.text.lower() not in stop_words:\n",
    "            tokens.append(word.text.lower())\n",
    "\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_spacy_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15382    [gt, ingen, virker, sjove, \\n\\n, kun,  \\n, tar...\n",
       "19594    [dude, you, re, such, huge, influence, and, fa...\n",
       "8085     [wow, fejrer, lige, lillebrors, √∏delagte, forh...\n",
       "2113     [l√¶ser, bunden, artiklen, s√•, sager, sj√¶ldne, ...\n",
       "16826    [glad, you, like, it, here, agree, on, your, t...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn tokenizer (fra CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer tokenizer funktion\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "def tokenizer_sklearn(text):\n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "    \n",
    "    words = tokenizer(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (len(word) < 2):\n",
    "            continue\n",
    "        if word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15382    [gt, ingen, virker, sjove, kun, tar, sp√∏g, sp√∏...\n",
       "19594    [dude, you, re, such, huge, influence, and, fa...\n",
       "8085     [wow, fejrer, lige, lillebrors, √∏delagte, forh...\n",
       "2113     [l√¶ser, bunden, artiklen, s√•, sager, sj√¶ldne, ...\n",
       "16826    [glad, you, like, it, here, agree, on, your, t...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sammenligning af tokenizers\n",
    "\n",
    "Nedenst√•ende kode opretter test-funktion for hver tokenizer. Test-funktionen k√∏rer tokenizeren p√• 100 reddit posts og viser, hvor lang tid tokenization tager i sekunder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-18 11:07:46 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "| lemma     | ddt     |\n",
      "| depparse  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-18 11:07:46 INFO: Use device: cpu\n",
      "2021-03-18 11:07:46 INFO: Loading: tokenize\n",
      "2021-03-18 11:07:46 INFO: Loading: pos\n",
      "2021-03-18 11:07:46 INFO: Loading: lemma\n",
      "2021-03-18 11:07:46 INFO: Loading: depparse\n",
      "2021-03-18 11:07:47 INFO: Done loading processors!\n",
      "2021-03-18 11:07:47 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "=======================\n",
      "\n",
      "2021-03-18 11:07:47 INFO: Use device: cpu\n",
      "2021-03-18 11:07:47 INFO: Loading: tokenize\n",
      "2021-03-18 11:07:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "\n",
    "nlp_stanza = stanza.Pipeline('da')\n",
    "nlp_stanza_simple = stanza.Pipeline('da', processors = 'tokenize')\n",
    "nlp_spacy_simple = spacy.load(\"da_core_news_sm\")\n",
    "sklearn_tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "def tokenizer_stanza(text, nlp = nlp_stanza): # Definerer funktion ud fra koden fra tidligere    \n",
    "\n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.lemma) < 2):\n",
    "                continue\n",
    "            if (word.pos in pos_tags) and (word.lemma not in stop_words):\n",
    "                tokens.append(word.lemma)\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "def tokenizer_stanza_simple(text, nlp = nlp_stanza_simple): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if (len(word.text) < 2):\n",
    "                continue\n",
    "            if word.text.lower() not in stop_words:\n",
    "                tokens.append(word.text.lower())\n",
    "                \n",
    "    return(tokens)\n",
    "\n",
    "def tokenizer_spacy_simple(text, nlp = nlp_spacy_simple): # Definerer funktion ud fra koden fra tidligere\n",
    "    \n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "\n",
    "    doc = nlp.tokenizer(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.text) < 2):\n",
    "            continue\n",
    "        if word.text.lower() not in stop_words:\n",
    "            tokens.append(word.text.lower())\n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "def tokenizer_sklearn(text, tokenizer = sklearn_tokenizer):\n",
    "    stop_words = list(nlp.Defaults.stop_words)\n",
    "    \n",
    "    words = tokenizer(text)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        if (len(word) < 2):\n",
    "            continue\n",
    "        if word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    \n",
    "    return(tokens)\n",
    "\n",
    "def stanza_full_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza)\n",
    "    \n",
    "    print(\"stanza full: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def stanza_simple_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_stanza_simple)\n",
    "    \n",
    "    print(\"stanza simple: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def spacy_simple_tester():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_spacy_simple)\n",
    "    \n",
    "    print(\"spacy simple: {0:.2f} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "def sklearn_tester():\n",
    "    start_time = time.time()\n",
    "          \n",
    "    reddit_sample = reddit_df.sample(100, random_state = 142)\n",
    "    reddit_sample['tokens'] = reddit_sample['comment_body'].apply(tokenizer_sklearn)\n",
    "    \n",
    "    print(\"sklearn: {0:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza full: 17.05 seconds\n",
      "stanza simple: 5.80 seconds\n",
      "spacy simple: 0.10 seconds\n",
      "sklearn: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "stanza_full_tester()\n",
    "stanza_simple_tester()\n",
    "spacy_simple_tester()\n",
    "sklearn_tester()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordopt√¶lling med vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagr kommentarer i objekt for sig\n",
    "\n",
    "comments = list(reddit_df['comment_body'])\n",
    "\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27918"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Countvectorizer p√• kommentarer - r√• tekst\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "# Konverter fittet vectorizer til array\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Konverter array til document-term matrix\n",
    "\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000001</th>\n",
       "      <th>0000034</th>\n",
       "      <th>000008</th>\n",
       "      <th>000008_da</th>\n",
       "      <th>00001</th>\n",
       "      <th>0001</th>\n",
       "      <th>000km</th>\n",
       "      <th>000kr</th>\n",
       "      <th>...</th>\n",
       "      <th>≈õwiatowƒÖ</th>\n",
       "      <th>∆®i</th>\n",
       "      <th> á…âw</th>\n",
       "      <th>Œºm</th>\n",
       "      <th>œ±ni ú…âŒ≥…ø…òv«ù</th>\n",
       "      <th>‡≤†_‡≤†</th>\n",
       "      <th>Âä†Ê≤π</th>\n",
       "      <th>ËÄÅÂ§ñ</th>\n",
       "      <th>Ôæü„ÉÆÔæü</th>\n",
       "      <th>ùì∑ùì≤ùì¨ùìÆ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 61478 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000001  0000034  000008  000008_da  00001  0001  000km  000kr  \\\n",
       "0   0    0        0        0       0          0      0     0      0      0   \n",
       "1   0    0        0        0       0          0      0     0      0      0   \n",
       "2   0    0        0        0       0          0      0     0      0      0   \n",
       "3   0    0        0        0       0          0      0     0      0      0   \n",
       "4   0    0        0        0       0          0      0     0      0      0   \n",
       "\n",
       "   ...  ≈õwiatowƒÖ  ∆®i   á…âw  Œºm  œ±ni ú…âŒ≥…ø…òv«ù  ‡≤†_‡≤†  Âä†Ê≤π  ËÄÅÂ§ñ  Ôæü„ÉÆÔæü  ùì∑ùì≤ùì¨ùìÆ  \n",
       "0  ...         0   0    0   0           0    0   0   0    0     0  \n",
       "1  ...         0   0    0   0           0    0   0   0    0     0  \n",
       "2  ...         0   0    0   0           0    0   0   0    0     0  \n",
       "3  ...         0   0    0   0           0    0   0   0    0     0  \n",
       "4  ...         0   0    0   0           0    0   0   0    0     0  \n",
       "\n",
       "[5 rows x 61478 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "er      43383\n",
       "det     41008\n",
       "at      38863\n",
       "og      26005\n",
       "ikke    23544\n",
       "en      20117\n",
       "jeg     19798\n",
       "der     17374\n",
       "p√•      15883\n",
       "har     15509\n",
       "til     15215\n",
       "for     15075\n",
       "s√•      14923\n",
       "de      14857\n",
       "af      12804\n",
       "med     12352\n",
       "du      12218\n",
       "som     11294\n",
       "kan     10505\n",
       "den     10003\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opt√¶lling af ord p√• tv√¶rs af dokumenter\n",
    "\n",
    "word_count = df.sum()\n",
    "word_count.sort_values(ascending = False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConuntVectorizer med stopord og dokumentgr√¶nser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "danmark         3436\n",
       "folk            3159\n",
       "godt            2764\n",
       "and             2219\n",
       "tror            2110\n",
       "ja              1740\n",
       "mennesker       1293\n",
       "mener           1241\n",
       "usa             1118\n",
       "nej             1097\n",
       "danske          1089\n",
       "racisme         1078\n",
       "alts√•           1042\n",
       "faktisk          996\n",
       "bedre            988\n",
       "tid              963\n",
       "mod              926\n",
       "dag              925\n",
       "denmark          922\n",
       "m√•de             908\n",
       "giver            904\n",
       "lande            896\n",
       "langt            869\n",
       "dansk            864\n",
       "g√•               843\n",
       "penge            828\n",
       "arbejde          823\n",
       "komme            809\n",
       "selvf√∏lgelig     793\n",
       "tager            790\n",
       "blevet           786\n",
       "eu               764\n",
       "bruge            738\n",
       "gang             730\n",
       "gerne            730\n",
       "hvilket          718\n",
       "dit              716\n",
       "finde            711\n",
       "10               688\n",
       "reddit           686\n",
       "corona           682\n",
       "stor             680\n",
       "land             677\n",
       "kina             638\n",
       "st√•r             634\n",
       "on               629\n",
       "virker           626\n",
       "tak              625\n",
       "danskere         618\n",
       "eks              613\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indl√¶ser spacy for at bruge spacy stopordsliste\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "custom_stops = ['gt', 'bare', 'the', 'to', 'n√•r', 'https', 'helt', 'of', 'se', 'in', 'www', 'is', 'you', 'dk', 'f√•r', 'com', 'ret', 'it', 'that', '√•r', 'siger',\n",
    "               'hele', 'g√•r', 'ting', 'ser', 'del', 'vel', 'tage', 'set', 'are', 'be', 'not', 'but', 'amp']\n",
    "\n",
    "stops = list(nlp.Defaults.stop_words) + custom_stops\n",
    "\n",
    "# Indstiller vectorizer - stopord og maksimalt antal dokumenter, ord m√• indg√• i (max. 70%)\n",
    "vectorizer = CountVectorizer(stop_words = stops, max_df = 0.7)\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter array til document-term matrix\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Ordopt√¶lling\n",
    "word_count = df.sum()\n",
    "word_count.sort_values(ascending = False)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative v√¶gtning af ord: Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "danmark         11813.412720\n",
       "folk            10874.034857\n",
       "and              9584.653471\n",
       "godt             9530.333742\n",
       "tror             7848.233386\n",
       "ja               6703.267100\n",
       "mennesker        5559.575015\n",
       "mener            5289.866531\n",
       "racisme          5163.357913\n",
       "usa              5010.086488\n",
       "danske           4865.021211\n",
       "nej              4738.334771\n",
       "alts√•            4555.847990\n",
       "faktisk          4414.718427\n",
       "bedre            4413.811714\n",
       "denmark          4320.475777\n",
       "tid              4308.782903\n",
       "mod              4258.206324\n",
       "dag              4236.810999\n",
       "lande            4170.913690\n",
       "m√•de             4096.922420\n",
       "giver            4095.301164\n",
       "dansk            4057.372288\n",
       "langt            3976.967716\n",
       "eu               3940.046972\n",
       "penge            3926.208068\n",
       "arbejde          3900.016426\n",
       "g√•               3859.059115\n",
       "komme            3725.494318\n",
       "blevet           3669.822075\n",
       "selvf√∏lgelig     3669.776803\n",
       "tager            3667.692931\n",
       "bruge            3490.581350\n",
       "reddit           3484.937455\n",
       "gang             3462.615497\n",
       "gerne            3445.155771\n",
       "dit              3434.019198\n",
       "hvilket          3416.641124\n",
       "finde            3375.726902\n",
       "10               3355.712343\n",
       "corona           3350.463976\n",
       "kina             3317.084381\n",
       "land             3303.232260\n",
       "stor             3255.957875\n",
       "on               3200.725905\n",
       "danskere         3097.727664\n",
       "st√•r             3093.425780\n",
       "virker           3043.635738\n",
       "we               3040.169044\n",
       "tak              3032.417713\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf-idf vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "custom_stops = ['gt', 'bare', 'the', 'to', 'n√•r', 'https', 'helt', 'of', 'se', 'in', 'www', 'is', 'you', 'dk', 'f√•r', 'com', 'ret', 'it', 'that', '√•r', 'siger',\n",
    "               'hele', 'g√•r', 'ting', 'ser', 'del', 'vel', 'tage', 'set', 'are', 'be', 'not', 'but', 'amp']\n",
    "\n",
    "stops = list(nlp.Defaults.stop_words) + custom_stops\n",
    "\n",
    "# Indstil tfidf vectorizer - samme indstillinger som f√∏r\n",
    "vectorizer = TfidfVectorizer(stop_words = stops, max_df = 0.7, norm = False)\n",
    "transformed_documents = vectorizer.fit_transform(comments)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter array til document-term matrix\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Ordopt√¶lling\n",
    "word_tfidfsum = df.sum()\n",
    "word_tfidfsum.sort_values(ascending = False)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf vectorizer p√• eksisterende tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion brugt til at tokenize data\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"da_core_news_sm\", disable = ['parser', 'ner', 'textcat'])\n",
    "\n",
    "def tokenizer_spacy(text):\n",
    "    custom_stops = ['gt', 'bare', 'the', 'to', 'n√•r', 'https', 'helt', 'of', 'se', 'in', 'www', 'is', 'you', 'dk', 'f√•r', 'com', 'ret', 'it', 'that', '√•r', 'siger',\n",
    "               'hele', 'g√•r', 'ting', 'ser', 'del', 'vel', 'tage', 'set', 'are', 'be', 'not', 'but', 'amp']\n",
    "    stop_words = list(nlp.Defaults.stop_words) + custom_stops\n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc:\n",
    "        if (len(word.lemma_) == 1):\n",
    "            continue\n",
    "        if (word.pos_ in pos_tags) and (word.lemma_.lower() not in stop_words):\n",
    "            tokens.append(word.lemma_.lower())\n",
    "                \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['comment_tokens'] = reddit_df['comment_body'].apply(tokenizer_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner kopi af data\n",
    "\n",
    "reddit_df_tokenized = reddit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evt. indl√¶s allerede eksisterende tokenized data\n",
    "#import ast\n",
    "#reddit_df_tokenized = pd.read_csv(\"https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark_q=danmark_01012020-30062020_long_filtered_tokenized.zip\")\n",
    "#reddit_df_tokenized['tokens'] = reddit_df_tokenized['tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "reddit_df_tokenized = reddit_df_tokenized.loc[reddit_df_tokenized['comment_tokens'].apply(lambda tokens: len(tokens) > 1), :]\n",
    "\n",
    "# Lagr kommentarer for sig\n",
    "comments_tokens = list(reddit_df_tokenized['comment_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "danmark       11864.229736\n",
       "folk          11332.328445\n",
       "mangen         8754.827562\n",
       "land           8188.696520\n",
       "stor           7813.605367\n",
       "dansk          7267.325863\n",
       "dag            6757.897987\n",
       "menneske       6211.115318\n",
       "tid            5665.105386\n",
       "sted           5524.909331\n",
       "problem        5522.579739\n",
       "gang           5410.667171\n",
       "racisme        5068.948012\n",
       "usa            4853.015416\n",
       "m√•de           4584.348719\n",
       "megen          4562.519161\n",
       "samfund        4542.027532\n",
       "penge          4400.695436\n",
       "dansker        4239.988622\n",
       "person         4221.863569\n",
       "denmark        3829.568910\n",
       "mand           3779.783923\n",
       "enig           3599.114109\n",
       "verden         3389.727459\n",
       "kina           3357.457118\n",
       "grund          3345.828030\n",
       "barn           3317.060691\n",
       "rette          3262.258413\n",
       "uge            3246.863997\n",
       "arbejde        3198.537319\n",
       "virksomhed     3134.775243\n",
       "h√∏j            3120.887727\n",
       "eu             3098.205092\n",
       "mening         3092.826105\n",
       "side           3031.313043\n",
       "tak            2992.252299\n",
       "stat           2981.849240\n",
       "parre          2963.441992\n",
       "situation      2937.843140\n",
       "sort           2883.944961\n",
       "forhold        2880.900899\n",
       "eksempel       2832.376024\n",
       "this           2800.192420\n",
       "sidste         2799.651721\n",
       "skatte         2774.685049\n",
       "sverige        2755.960954\n",
       "system         2726.587439\n",
       "lov            2711.741177\n",
       "by             2664.157340\n",
       "sp√∏rgsm√•l      2650.324731\n",
       "dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tfidfvectorizer p√• tokens\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dummyfunktion - bruges som tokenizer-funktion i vectorizer\n",
    "def return_tokens(tokens):\n",
    "    return tokens\n",
    "\n",
    "# Indstiller vectorizer med brug af dummyfunktion (returnerer blot tokens, da data allerede er tokenized)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=return_tokens,\n",
    "    preprocessor=return_tokens,\n",
    "    token_pattern=None,\n",
    "    norm = False)\n",
    "\n",
    "# Fitter vectorizer\n",
    "transformed_documents = vectorizer.fit_transform(comments_tokens)\n",
    "\n",
    "# Konverter til array\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter til document-term matrix\n",
    "df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Ordopt√¶lling\n",
    "word_tfidfsum = df.sum().sort_values(ascending = False)\n",
    "word_tfidfsum[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fra tekst til features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy-variabel for upvoted/ikke upvoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner kopi af data\n",
    "\n",
    "reddit_df_rf = reddit_df_tokenized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "Name: comment_score, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tjekker indhold af variabel \"comment_score\"\n",
    "reddit_df_rf['comment_score'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner dummy for upvoted\n",
    "reddit_df_rf['comment_upvoted'] = reddit_df_rf['comment_score'] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "Name: comment_upvoted, dtype: bool"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tjekker indhold af ny variabel\n",
    "reddit_df_rf['comment_upvoted'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    23283\n",
       "True      2827\n",
       "Name: comment_upvoted, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opt√¶lling p√• ny variabel\n",
    "reddit_df_rf['comment_upvoted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    25442\n",
       "True       668\n",
       "Name: comment_downvoted, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variabel for downvoted\n",
    "reddit_df_rf['comment_downvoted'] = reddit_df_rf['comment_score'] < 1\n",
    "\n",
    "# Opt√¶lling\n",
    "reddit_df_rf['comment_downvoted'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra tekst til dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner ordliste af 50 mest hyppige ord baseret p√• tfidf fra tidligere\n",
    "top_words = list(word_tfidfsum.index[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danmark',\n",
       " 'folk',\n",
       " 'mangen',\n",
       " 'land',\n",
       " 'stor',\n",
       " 'dansk',\n",
       " 'dag',\n",
       " 'menneske',\n",
       " 'tid',\n",
       " 'sted',\n",
       " 'problem',\n",
       " 'gang',\n",
       " 'racisme',\n",
       " 'usa',\n",
       " 'm√•de',\n",
       " 'megen',\n",
       " 'samfund',\n",
       " 'penge',\n",
       " 'dansker',\n",
       " 'person',\n",
       " 'denmark',\n",
       " 'mand',\n",
       " 'enig',\n",
       " 'verden',\n",
       " 'kina',\n",
       " 'grund',\n",
       " 'barn',\n",
       " 'rette',\n",
       " 'uge',\n",
       " 'arbejde',\n",
       " 'virksomhed',\n",
       " 'h√∏j',\n",
       " 'eu',\n",
       " 'mening',\n",
       " 'side',\n",
       " 'tak',\n",
       " 'stat',\n",
       " 'parre',\n",
       " 'situation',\n",
       " 'sort',\n",
       " 'forhold',\n",
       " 'eksempel',\n",
       " 'this',\n",
       " 'sidste',\n",
       " 'skatte',\n",
       " 'sverige',\n",
       " 'system',\n",
       " 'lov',\n",
       " 'by',\n",
       " 'sp√∏rgsm√•l']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop igennem hvert ord i topwords og dan dummyvariabel for hvorvidt ord indg√•r i kommentar eller ej (ud fra token-liste)\n",
    "for word in top_words:\n",
    "    colname = \"token_{}\".format(word) # Denne linje giver dummyvariabel for ord pr√¶fix \"token_\"\n",
    "    reddit_df_rf[colname] = reddit_df_rf['comment_tokens'].apply(lambda tokens: int(word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_author</th>\n",
       "      <th>post_created_utc</th>\n",
       "      <th>post_domain</th>\n",
       "      <th>post_full_link</th>\n",
       "      <th>post_gildings</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_is_original_content</th>\n",
       "      <th>post_is_reddit_media_domain</th>\n",
       "      <th>post_locked</th>\n",
       "      <th>post_media_only</th>\n",
       "      <th>...</th>\n",
       "      <th>token_forhold</th>\n",
       "      <th>token_eksempel</th>\n",
       "      <th>token_this</th>\n",
       "      <th>token_sidste</th>\n",
       "      <th>token_skatte</th>\n",
       "      <th>token_sverige</th>\n",
       "      <th>token_system</th>\n",
       "      <th>token_lov</th>\n",
       "      <th>token_by</th>\n",
       "      <th>token_sp√∏rgsm√•l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LotusLemmedasker</td>\n",
       "      <td>1577901072</td>\n",
       "      <td>self.Denmark</td>\n",
       "      <td>https://www.reddit.com/r/Denmark/comments/eil6...</td>\n",
       "      <td>{}</td>\n",
       "      <td>eil6k2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LotusLemmedasker</td>\n",
       "      <td>1577901072</td>\n",
       "      <td>self.Denmark</td>\n",
       "      <td>https://www.reddit.com/r/Denmark/comments/eil6...</td>\n",
       "      <td>{}</td>\n",
       "      <td>eil6k2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LotusLemmedasker</td>\n",
       "      <td>1577901072</td>\n",
       "      <td>self.Denmark</td>\n",
       "      <td>https://www.reddit.com/r/Denmark/comments/eil6...</td>\n",
       "      <td>{}</td>\n",
       "      <td>eil6k2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LotusLemmedasker</td>\n",
       "      <td>1577901072</td>\n",
       "      <td>self.Denmark</td>\n",
       "      <td>https://www.reddit.com/r/Denmark/comments/eil6...</td>\n",
       "      <td>{}</td>\n",
       "      <td>eil6k2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LotusLemmedasker</td>\n",
       "      <td>1577901072</td>\n",
       "      <td>self.Denmark</td>\n",
       "      <td>https://www.reddit.com/r/Denmark/comments/eil6...</td>\n",
       "      <td>{}</td>\n",
       "      <td>eil6k2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_author  post_created_utc   post_domain  \\\n",
       "3  LotusLemmedasker        1577901072  self.Denmark   \n",
       "4  LotusLemmedasker        1577901072  self.Denmark   \n",
       "5  LotusLemmedasker        1577901072  self.Denmark   \n",
       "6  LotusLemmedasker        1577901072  self.Denmark   \n",
       "7  LotusLemmedasker        1577901072  self.Denmark   \n",
       "\n",
       "                                      post_full_link post_gildings post_id  \\\n",
       "3  https://www.reddit.com/r/Denmark/comments/eil6...            {}  eil6k2   \n",
       "4  https://www.reddit.com/r/Denmark/comments/eil6...            {}  eil6k2   \n",
       "5  https://www.reddit.com/r/Denmark/comments/eil6...            {}  eil6k2   \n",
       "6  https://www.reddit.com/r/Denmark/comments/eil6...            {}  eil6k2   \n",
       "7  https://www.reddit.com/r/Denmark/comments/eil6...            {}  eil6k2   \n",
       "\n",
       "   post_is_original_content  post_is_reddit_media_domain  post_locked  \\\n",
       "3                     False                        False        False   \n",
       "4                     False                        False        False   \n",
       "5                     False                        False        False   \n",
       "6                     False                        False        False   \n",
       "7                     False                        False        False   \n",
       "\n",
       "   post_media_only  ...  token_forhold  token_eksempel  token_this  \\\n",
       "3            False  ...              0               0           0   \n",
       "4            False  ...              0               0           0   \n",
       "5            False  ...              0               0           0   \n",
       "6            False  ...              0               0           0   \n",
       "7            False  ...              0               0           0   \n",
       "\n",
       "  token_sidste  token_skatte  token_sverige  token_system token_lov  token_by  \\\n",
       "3            0             0              0             0         0         0   \n",
       "4            0             0              0             0         0         0   \n",
       "5            0             0              0             0         0         0   \n",
       "6            0             0              0             0         0         0   \n",
       "7            0             0              0             0         0         0   \n",
       "\n",
       "   token_sp√∏rgsm√•l  \n",
       "3                0  \n",
       "4                0  \n",
       "5                0  \n",
       "6                0  \n",
       "7                0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_danmark',\n",
       " 'token_folk',\n",
       " 'token_mangen',\n",
       " 'token_land',\n",
       " 'token_stor',\n",
       " 'token_dansk',\n",
       " 'token_dag',\n",
       " 'token_menneske',\n",
       " 'token_tid',\n",
       " 'token_sted',\n",
       " 'token_problem',\n",
       " 'token_gang',\n",
       " 'token_racisme',\n",
       " 'token_usa',\n",
       " 'token_m√•de',\n",
       " 'token_megen',\n",
       " 'token_samfund',\n",
       " 'token_penge',\n",
       " 'token_dansker',\n",
       " 'token_person',\n",
       " 'token_denmark',\n",
       " 'token_mand',\n",
       " 'token_enig',\n",
       " 'token_verden',\n",
       " 'token_kina',\n",
       " 'token_grund',\n",
       " 'token_barn',\n",
       " 'token_rette',\n",
       " 'token_uge',\n",
       " 'token_arbejde',\n",
       " 'token_virksomhed',\n",
       " 'token_h√∏j',\n",
       " 'token_eu',\n",
       " 'token_mening',\n",
       " 'token_side',\n",
       " 'token_tak',\n",
       " 'token_stat',\n",
       " 'token_parre',\n",
       " 'token_situation',\n",
       " 'token_sort',\n",
       " 'token_forhold',\n",
       " 'token_eksempel',\n",
       " 'token_this',\n",
       " 'token_sidste',\n",
       " 'token_skatte',\n",
       " 'token_sverige',\n",
       " 'token_system',\n",
       " 'token_lov',\n",
       " 'token_by',\n",
       " 'token_sp√∏rgsm√•l']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tjek dummyvariable for tekst\n",
    "[column for column in reddit_df_rf.columns if column.startswith('token_')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
