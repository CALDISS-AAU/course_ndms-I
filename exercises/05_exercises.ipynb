{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ØVELSE 1: Simpel teksthåndtering\n",
    "\n",
    "I øvelserne i dag skal i arbejde med et datasæt bestående af kommentarer fra reddit. Alle kommentarer er taget fra posts på r/denmark (reddit.com/r/denmark) fra 1/3-8/3 2021.\n",
    "\n",
    "1. Indlæs data som en pandas data frame\n",
    "    - Link til data: https://raw.githubusercontent.com/CALDISS-AAU/course_ndms-I/master/datasets/reddit_rdenmark-comments_01032021-08032021_long.csv\n",
    "2. Dan et subset bestående af alle kommentarer, der nævner \"menneskerettigheder\" (kommentarteksten er i kolonnen `comment_body`). Hvor mange kommentarer er der?\n",
    "\n",
    "**Bonus**\n",
    "- Kan du udregne gennemsnitsscore for de kommentarer, der nævner menneskerettigheder? (score fremgår af kolonnen `comment_score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ØVELSE 2: Tokenization\n",
    "\n",
    "Prøv at undersøge tekstindholdet af et af opslagene i redditdata: \n",
    "\n",
    "```\n",
    "post = reddit_df.loc[503, 'post_selftext']\n",
    "```\n",
    "\n",
    "1. Tokenize opslaget - enten med en funktion eller trin for trin\n",
    "2. Lav en ordoptælling ved at konvertere jeres tokens til en pandas series og bruge `.value_counts()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ØVELSE 3: Meningsfulde tokens med stanza\n",
    "\n",
    "Prøv at undersøge tekstindholdet af samme reddit opslag som før, men denne gang med brug af `stanza`: \n",
    "\n",
    "```\n",
    "post = reddit_df.loc[503, 'post_selftext']\n",
    "```\n",
    "\n",
    "1. Skriv en tokenizer funktion, der bruger `stanza`\n",
    "2. Tokenize opslaget med funktionen\n",
    "3. Lav igen en ordoptælling ved at konvertere jeres tokens til en pandas series og bruge `.value_counts()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ØVELSE 4: Tidy text data (reddit data)\n",
    "\n",
    "Du skal nu anvende din funktion fra før på alle kommentarerne i reddit datasættet (kolonnen `comment_body`)\n",
    "\n",
    "1. Brug `.apply()` til at anvende din tokenizer funktion på hele reddit datasættet til at lave en tokens kolonne (det kan være en god ide lige at teste funktionen med en enkelt kommentar først)\n",
    "2. Brug `.explode()` til at konvertere data til et tidy format\n",
    "3. Brug `.value_counts()` til at optælle tokens\n",
    "4. Undersøg, hvor mange gange coronavirus er nævnt (tænkt gerne synonymer med!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ØVELSE 5: Topic models i Python (reddit data)\n",
    "\n",
    "I skal nu lave en topic model for reddit kommentarerne.\n",
    "\n",
    "1. Dan et \"gensim corpus\" ud fra reddit kommentarerne (`comment_body`)\n",
    "2. Dan en topic model over kommentarerne (bestem selv antal topics)\n",
    "3. Inspicér de mest sandsynlige ord i hvert topic (`lda_model.show_topics(formatted=False)`)\n",
    "4. Hvad kan I bruge modellen til?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
